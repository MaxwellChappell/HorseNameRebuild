{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_length = 1    # The step length we take to get our samples from our corpus\n",
    "epochs = 100       # Number of times we train on our full data\n",
    "batch_size = 32    # Data samples in each training step\n",
    "latent_dim = 64    # Size of our LSTM\n",
    "dropout_rate = 0.2 # Regularization with dropout\n",
    "model_path = os.path.realpath('./horse_gen_model.h5') # Location for the model\n",
    "load_model = False # Enable loading model from disk\n",
    "store_model = True # Store model to disk after training\n",
    "verbosity = 1      # Print result for each epoch\n",
    "gen_amount = 1000    # How many "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load names\n",
    "df = pd.read_csv(\"preprocessed.csv\")\n",
    "name_list = df['Horses'].values.tolist()\n",
    "name_list = [str(i) for i in name_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 36\n",
      "Corpus length: 1477353\n",
      "Number of names:  152290\n",
      "Longest name:  33\n"
     ]
    }
   ],
   "source": [
    "concat_names = '\\n'.join(name_list).lower()\n",
    "\n",
    "# Find all unique characters by using set()\n",
    "chars = sorted(list(set(concat_names)))\n",
    "num_chars = len(chars)\n",
    "\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "# Use longest name length as our sequence window\n",
    "max_sequence_length = max([len(name) for name in name_list])\n",
    "\n",
    "print('Total chars: {}'.format(num_chars))\n",
    "print('Corpus length:', len(concat_names))\n",
    "print('Number of names: ', len(name_list))\n",
    "print('Longest name: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1477320\n",
      "First 10 sequences and next chars:\n",
      "X=[darley arabian godolphin arabian ]   y=[b]\n",
      "X=[arley arabian godolphin arabian b]   y=[y]\n",
      "X=[rley arabian godolphin arabian by]   y=[e]\n",
      "X=[ley arabian godolphin arabian bye]   y=[r]\n",
      "X=[ey arabian godolphin arabian byer]   y=[l]\n",
      "X=[y arabian godolphin arabian byerl]   y=[e]\n",
      "X=[ arabian godolphin arabian byerle]   y=[y]\n",
      "X=[arabian godolphin arabian byerley]   y=[ ]\n",
      "X=[rabian godolphin arabian byerley ]   y=[t]\n",
      "X=[abian godolphin arabian byerley t]   y=[u]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Loop over our data and extract pairs of sequances and next chars\n",
    "for i in range(0, len(concat_names) - max_sequence_length, step_length):\n",
    "    sequences.append(concat_names[i: i + max_sequence_length])\n",
    "    next_chars.append(concat_names[i + max_sequence_length])\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('X=[{}]   y=[{}]'.replace('\\n', ' ').format(sequences[i], next_chars[i]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1477320, 33, 36)\n",
      "Y shape: (1477320, 36)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=bool)\n",
    "Y = np.zeros((num_sequences, num_chars), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, char in enumerate(sequence):\n",
    "        X[i, j, char_to_index[char]] = 1\n",
    "    Y[i, char_to_index[next_chars[i]]] = 1\n",
    "    \n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 64)                25856     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 36)                2340      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,196\n",
      "Trainable params: 28,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, \n",
    "               input_shape=(max_sequence_length, num_chars),  \n",
    "               recurrent_dropout=dropout_rate))\n",
    "model.add(Dense(units=num_chars, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n",
      "Epoch 1/100\n",
      "46167/46167 [==============================] - 809s 17ms/step - loss: 2.2522\n",
      "Epoch 2/100\n",
      "46167/46167 [==============================] - 816s 18ms/step - loss: 2.2141\n",
      "Epoch 3/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 6.8855\n",
      "Epoch 4/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 5.0751\n",
      "Epoch 5/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.6776\n",
      "Epoch 6/100\n",
      "46167/46167 [==============================] - 821s 18ms/step - loss: 3.3207\n",
      "Epoch 7/100\n",
      "46167/46167 [==============================] - 807s 17ms/step - loss: 3.1463\n",
      "Epoch 8/100\n",
      "46167/46167 [==============================] - 804s 17ms/step - loss: 3.0746\n",
      "Epoch 9/100\n",
      "46167/46167 [==============================] - 812s 18ms/step - loss: 3.0871\n",
      "Epoch 10/100\n",
      "46167/46167 [==============================] - 811s 18ms/step - loss: 3.0889\n",
      "Epoch 11/100\n",
      "46167/46167 [==============================] - 809s 18ms/step - loss: 3.1106\n",
      "Epoch 12/100\n",
      "46167/46167 [==============================] - 807s 17ms/step - loss: 3.1276\n",
      "Epoch 13/100\n",
      "46167/46167 [==============================] - 805s 17ms/step - loss: 3.1312\n",
      "Epoch 14/100\n",
      "46167/46167 [==============================] - 809s 18ms/step - loss: 3.1375\n",
      "Epoch 15/100\n",
      "46167/46167 [==============================] - 809s 18ms/step - loss: 3.1305\n",
      "Epoch 16/100\n",
      "46167/46167 [==============================] - 809s 18ms/step - loss: 3.1268\n",
      "Epoch 17/100\n",
      "46167/46167 [==============================] - 810s 18ms/step - loss: 3.1283\n",
      "Epoch 18/100\n",
      "46167/46167 [==============================] - 817s 18ms/step - loss: 3.1329\n",
      "Epoch 19/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.1408\n",
      "Epoch 20/100\n",
      "46167/46167 [==============================] - 821s 18ms/step - loss: 3.1511\n",
      "Epoch 21/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.1662\n",
      "Epoch 22/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.1657\n",
      "Epoch 23/100\n",
      "46167/46167 [==============================] - 822s 18ms/step - loss: 3.1870\n",
      "Epoch 24/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.1993\n",
      "Epoch 25/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.2175\n",
      "Epoch 26/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.2505\n",
      "Epoch 27/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.2625\n",
      "Epoch 28/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.2719\n",
      "Epoch 29/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.2989\n",
      "Epoch 30/100\n",
      "46167/46167 [==============================] - 821s 18ms/step - loss: 3.3100\n",
      "Epoch 31/100\n",
      "46167/46167 [==============================] - 822s 18ms/step - loss: 3.3145\n",
      "Epoch 32/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.3605\n",
      "Epoch 33/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.3717\n",
      "Epoch 34/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.3845\n",
      "Epoch 35/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.3686\n",
      "Epoch 36/100\n",
      "46167/46167 [==============================] - 841s 18ms/step - loss: 3.3410\n",
      "Epoch 37/100\n",
      "46167/46167 [==============================] - 845s 18ms/step - loss: 3.3511\n",
      "Epoch 38/100\n",
      "46167/46167 [==============================] - 830s 18ms/step - loss: 3.3740\n",
      "Epoch 39/100\n",
      "46167/46167 [==============================] - 830s 18ms/step - loss: 3.3613\n",
      "Epoch 40/100\n",
      "46167/46167 [==============================] - 832s 18ms/step - loss: 3.3754\n",
      "Epoch 41/100\n",
      "46167/46167 [==============================] - 832s 18ms/step - loss: 3.3659\n",
      "Epoch 42/100\n",
      "46167/46167 [==============================] - 828s 18ms/step - loss: 3.3535\n",
      "Epoch 43/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.3742\n",
      "Epoch 44/100\n",
      "46167/46167 [==============================] - 829s 18ms/step - loss: 3.3862\n",
      "Epoch 45/100\n",
      "46167/46167 [==============================] - 834s 18ms/step - loss: 3.3695\n",
      "Epoch 46/100\n",
      "46167/46167 [==============================] - 828s 18ms/step - loss: 3.3465\n",
      "Epoch 47/100\n",
      "46167/46167 [==============================] - 829s 18ms/step - loss: 3.3255\n",
      "Epoch 48/100\n",
      "46167/46167 [==============================] - 828s 18ms/step - loss: 3.3283\n",
      "Epoch 49/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.3226\n",
      "Epoch 50/100\n",
      "46167/46167 [==============================] - 882s 19ms/step - loss: 3.3194\n",
      "Epoch 51/100\n",
      "46167/46167 [==============================] - 832s 18ms/step - loss: 3.3154\n",
      "Epoch 52/100\n",
      "46167/46167 [==============================] - 831s 18ms/step - loss: 3.3178\n",
      "Epoch 53/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.3301\n",
      "Epoch 54/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.3133\n",
      "Epoch 55/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.3076\n",
      "Epoch 56/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.3275\n",
      "Epoch 57/100\n",
      "46167/46167 [==============================] - 835s 18ms/step - loss: 3.3394\n",
      "Epoch 58/100\n",
      "46167/46167 [==============================] - 836s 18ms/step - loss: 3.3545\n",
      "Epoch 59/100\n",
      "46167/46167 [==============================] - 835s 18ms/step - loss: 3.3553\n",
      "Epoch 60/100\n",
      "46167/46167 [==============================] - 838s 18ms/step - loss: 3.3706\n",
      "Epoch 61/100\n",
      "46167/46167 [==============================] - 836s 18ms/step - loss: 3.3617\n",
      "Epoch 62/100\n",
      "46167/46167 [==============================] - 849s 18ms/step - loss: 3.3749\n",
      "Epoch 63/100\n",
      "46167/46167 [==============================] - 852s 18ms/step - loss: 3.3856\n",
      "Epoch 64/100\n",
      "46167/46167 [==============================] - 843s 18ms/step - loss: 3.3900\n",
      "Epoch 65/100\n",
      "46167/46167 [==============================] - 847s 18ms/step - loss: 3.4193\n",
      "Epoch 66/100\n",
      "46167/46167 [==============================] - 831s 18ms/step - loss: 3.4219\n",
      "Epoch 67/100\n",
      "46167/46167 [==============================] - 923s 20ms/step - loss: 3.4227\n",
      "Epoch 68/100\n",
      "46167/46167 [==============================] - 1051s 23ms/step - loss: 3.4293\n",
      "Epoch 69/100\n",
      "46167/46167 [==============================] - 832s 18ms/step - loss: 3.4297\n",
      "Epoch 70/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.4656\n",
      "Epoch 71/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.4905\n",
      "Epoch 72/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.4858\n",
      "Epoch 73/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.5075\n",
      "Epoch 74/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.4900\n",
      "Epoch 75/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.4930\n",
      "Epoch 76/100\n",
      "46167/46167 [==============================] - 821s 18ms/step - loss: 3.5276\n",
      "Epoch 77/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.5320\n",
      "Epoch 78/100\n",
      "46167/46167 [==============================] - 825s 18ms/step - loss: 3.5216\n",
      "Epoch 79/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.5606\n",
      "Epoch 80/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.5249\n",
      "Epoch 81/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.4995\n",
      "Epoch 82/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.5325\n",
      "Epoch 83/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.5303\n",
      "Epoch 84/100\n",
      "46167/46167 [==============================] - 823s 18ms/step - loss: 3.5220\n",
      "Epoch 85/100\n",
      "46167/46167 [==============================] - 822s 18ms/step - loss: 3.5179\n",
      "Epoch 86/100\n",
      "46167/46167 [==============================] - 822s 18ms/step - loss: 3.5352\n",
      "Epoch 87/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.5281\n",
      "Epoch 88/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.5345\n",
      "Epoch 89/100\n",
      "46167/46167 [==============================] - 824s 18ms/step - loss: 3.5132\n",
      "Epoch 90/100\n",
      "46167/46167 [==============================] - 851s 18ms/step - loss: 3.5258\n",
      "Epoch 91/100\n",
      "46167/46167 [==============================] - 807s 17ms/step - loss: 3.5146\n",
      "Epoch 92/100\n",
      "46167/46167 [==============================] - 810s 18ms/step - loss: 3.5220\n",
      "Epoch 93/100\n",
      "46167/46167 [==============================] - 807s 17ms/step - loss: 3.5588\n",
      "Epoch 94/100\n",
      "46167/46167 [==============================] - 816s 18ms/step - loss: 3.5513\n",
      "Epoch 95/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.5450\n",
      "Epoch 96/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.5607\n",
      "Epoch 97/100\n",
      "46167/46167 [==============================] - 830s 18ms/step - loss: 3.5565\n",
      "Epoch 98/100\n",
      "46167/46167 [==============================] - 819s 18ms/step - loss: 3.5688\n",
      "Epoch 99/100\n",
      "46167/46167 [==============================] - 827s 18ms/step - loss: 3.5603\n",
      "Epoch 100/100\n",
      "46167/46167 [==============================] - 826s 18ms/step - loss: 3.5782\n",
      "Finished training - time elapsed: 1380.9324082811675 min\n",
      "Storing model at: C:\\Users\\mjcha\\projects\\horse\\horse_gen_model.h5\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model.load_weights(model_path)\n",
    "else:\n",
    "    start = time.time()\n",
    "    print('Start training for {} epochs'.format(epochs))\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbosity)\n",
    "    end = time.time()\n",
    "    print('Finished training - time elapsed:', (end - start)/60, 'min')\n",
    "    \n",
    "if store_model:\n",
    "    print('Storing model at:', model_path)\n",
    "    model.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 new names are being generated\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 0\n",
      "Generated 100\n",
      "Generated 200\n",
      "Generated 300\n",
      "Generated 400\n",
      "Generated 500\n",
      "Generated 600\n",
      "Generated 700\n",
      "Generated 800\n",
      "Generated 900\n",
      "Generated 1000\n"
     ]
    }
   ],
   "source": [
    "# Start sequence generation from end of the input sequence\n",
    "sequence = concat_names[-(max_sequence_length - 1):] + '\\n'\n",
    "\n",
    "new_names = []\n",
    "\n",
    "print('{} new names are being generated'.format(gen_amount))\n",
    "\n",
    "while len(new_names) < gen_amount:\n",
    "    \n",
    "    # Vectorize sequence for prediction\n",
    "    x = np.zeros((1, max_sequence_length, num_chars))\n",
    "    for i, char in enumerate(sequence):\n",
    "        x[0, i, char_to_index[char]] = 1\n",
    "\n",
    "    # Sample next char from predicted probabilities\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    probs /= probs.sum()\n",
    "    next_idx = np.random.choice(len(probs), p=probs)   \n",
    "    next_char = index_to_char[next_idx]   \n",
    "    \n",
    "    sequence = sequence[1:] + next_char\n",
    "\n",
    "    # New line means we have a new name\n",
    "    if next_char == '\\n':\n",
    "\n",
    "        gen_name = [name for name in sequence.split('\\n')][1]\n",
    "\n",
    "        # Never start name with two identical chars, could probably also\n",
    "        if len(gen_name) > 2 and gen_name[0] == gen_name[1]:\n",
    "            gen_name = gen_name[1:]\n",
    "\n",
    "        # Discard all names that are too short\n",
    "        if len(gen_name) > 2:\n",
    "            \n",
    "            # Only allow new and unique names\n",
    "            if gen_name not in name_list + new_names:\n",
    "                new_names.append(gen_name.capitalize())\n",
    "\n",
    "        if 0 == (len(new_names) % (gen_amount/ 10)):\n",
    "            print('Generated {}'.format(len(new_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 generated names:\n",
      "Dolapa\n",
      "Dolapa\n",
      "Dolapa\n",
      "Rd aer\n",
      "Rd aer\n",
      "Nt or hy wfi hteg\n",
      "Arrol\n",
      "Arrol\n",
      "Arrol\n",
      "Arrol\n"
     ]
    }
   ],
   "source": [
    "print_first_n = min(10, gen_amount)\n",
    "\n",
    "print('First {} generated names:'.format(print_first_n))\n",
    "for name in new_names[:print_first_n]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_output = '\\n'.join(sorted(new_names))\n",
    "output_path = os.path.realpath('generated_names.txt')\n",
    "\n",
    "with open(output_path, 'a') as f:\n",
    "    f.write(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f12dba4e2a944ff3c464dc2cdd9ede4d0c6e60e4895fd418f91d8aae8f7462d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
